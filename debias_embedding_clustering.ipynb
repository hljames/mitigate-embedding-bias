{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import random\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Currently the model only handles `batch_size=1`. As there are only 221 professions for debiasing, this is a non-issue, but this should be changed in a future version to accomodate larger sets of words to debias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 22000\n",
    "vector_dim = 100\n",
    "batch_size = 1\n",
    "k_knn = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_model = Word2Vec.load(\"english-wikipedia-articles-20170820-models/enwiki_2017_08_20_fasttext.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvs = wiki_model.wv.vectors[:vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a,b):\n",
    "    return 1 - spatial.distance.cosine(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     1,     2, ..., 21997, 21998, 21999])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inapp_gen_indices = np.load('inapp_gen_indices.npy')\n",
    "inapp_gen_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `debias_embedding_probabilistic.ipynb` for construction of gender_word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_word_pairs = np.load('gender_word_pairs.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the gender vector (see Bolukbasi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = np.mean([wiki_model.wv.get_vector(f) - wiki_model.wv.get_vector(m) for m,f in gender_word_pairs],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21872,)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases = np.array([cosine_similarity(wvs[i], g) for i in inapp_gen_indices])\n",
    "biases.shape\n",
    "#biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_neighbors = np.array([wiki_model.wv.index2word[ind] for ind in inapp_gen_indices[(biases.argsort()[::-1])[:500]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('female_neighbors', female_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_neighbors = np.array([wiki_model.wv.index2word[ind] for ind in inapp_gen_indices[(biases.argsort())[:500]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['warlord', 'abbot', 'chiefs', 'captain', 'sgt', 'simeon',\n",
       "       'colonel', 'umpire', 'muhammad', 'godfrey', 'lord', 'sack', 'davy',\n",
       "       'hitler', 'caesar', 'friar', 'brigadier', 'horde', 'sidekick',\n",
       "       'joker', 'pope', 'captaincy', 'wrath', 'capt', 'ruler',\n",
       "       'chieftain', 'judah', 'johnny', 'kirby', 'halfback', 'cromwell',\n",
       "       'crook', 'bat', 'conqueror', 'assassin', 'abraham', 'hooker',\n",
       "       'haig', 'bodyguard', 'wizard', 'napoleon', 'soldier', 'ibn',\n",
       "       'oswald', 'ezekiel', 'henry', 'robber', 'vizier', 'knight', 'buck',\n",
       "       'bishop', 'caliph', 'assassins', 'broncos', 'squad', 'goblin',\n",
       "       'crusader', 'mccoy', 'joe', 'nobleman', 'batsman', 'darius',\n",
       "       'sheriff', 'dick', 'chamberlain', 'barons', 'mlb', 'maj', 'jr',\n",
       "       'boss', 'john', 'kicker', 'bud', 'emperor', 'slain', 'pardon',\n",
       "       'hrer', 'duke', 'horseback', 'hank', 'gus', 'nfl', 'xi', 'mob',\n",
       "       'mr', 'wyatt', 'kirk', 'keeper', 'apostle', 'budd', 'striker',\n",
       "       'archbishop', 'sox', 'dictator', 'kick', 'bald', 'sword', 'luke',\n",
       "       'viceroy', 'hendrick', 'cowboys', 'burt', 'kidd', 'cavalry',\n",
       "       'dempsey', 'gorman', 'marshal', 'thief', 'jacobite', 'isaac',\n",
       "       'prophet', 'bench', 'teammate', 'redskins', 'abe', 'lieutenant',\n",
       "       'fullback', 'ox', 'hiram', 'butcher', 'foe', 'pius', 'jeremiah',\n",
       "       'goalkeeper', 'lucius', 'foreman', 'jimmy', 'enoch', 'charlemagne',\n",
       "       'grenadier', 'hugh', 'gaul', 'leroy', 'gough', 'tommy', 'jacob',\n",
       "       'joachim', 'pitcher', 'umar', 'royals', 'hogan', 'mercenary',\n",
       "       'jock', 'lt', 'fielder', 'genius', 'archduke', 'roy', 'jim',\n",
       "       'helm', 'julius', 'army', 'pir', 'burger', 'bowler', 'quarterback',\n",
       "       'trumpeter', 'rabbi', 'duel', 'isaiah', 'gurney', 'josiah',\n",
       "       'gaddafi', 'daredevil', 'bastard', 'joke', 'ibrahim', 'ephraim',\n",
       "       'saul', 'thug', 'franks', 'hero', 'maharaja', 'jack', 'rebel',\n",
       "       'bulldog', 'konrad', 'cardinals', 'policeman', 'sid', 'dodgers',\n",
       "       'posse', 'judas', 'roach', 'locke', 'robb', 'nba', 'defeat',\n",
       "       'khan', 'fist', 'lords', 'casimir', 'redshirt', 'obey', 'dave',\n",
       "       'who', 'boxer', 'crusaders', 'rhino', 'davey', 'batman', 'joseph',\n",
       "       'rook', 'drummer', 'sergeant', 'sax', 'alfonso', 'bey', 'fury',\n",
       "       'steward', 'ambrose', 'kid', 'bandleader', 'bowie', 'godwin',\n",
       "       'xiii', 'lew', 'gangster', 'tsar', 'hodgson', 'leo', 'cpt',\n",
       "       'rufus', 'willie', 'crockett', 'patriot', 'roger', 'oath',\n",
       "       'trooper', 'referee', 'knicks', 'yankees', 'akbar', 'billy',\n",
       "       'lamb', 'frontman', 'butch', 'benedict', 'george', 'mosley',\n",
       "       'basil', 'chief', 'god', 'cfl', 'batsmen', 'outlaw', 'commander',\n",
       "       'scout', 'ebert', 'alf', 'cowboy', 'seaman', 'leader', 'kemp',\n",
       "       'bobby', 'hector', 'philip', 'cubs', 'roderick', 'outfielder',\n",
       "       'cornerback', 'vicar', 'giants', 'defenceman', 'wicket', 'kenny',\n",
       "       'steelers', 'dso', 'handy', 'cyril', 'ignatius', 'vfl', 'pryor',\n",
       "       'elijah', 'warriors', 'johnson', 'colonels', 'lineman',\n",
       "       'linebacker', 'buddy', 'knights', 'hotspur', 'renegade', 'pilgrim',\n",
       "       'bahadur', 'doom', 'trumpet', 'satan', 'truce', 'yogi', 'duchy',\n",
       "       'headmaster', 'martyn', 'jerry', 'crusade', 'siegel', 'tackle',\n",
       "       'villain', 'cain', 'samurai', 'hawke', 'hoyt', 'gideon', 'eddie',\n",
       "       'duly', 'duck', 'gregg', 'englishman', 'ebenezer', 'burr', 'titus',\n",
       "       'heer', 'royalist', 'clergyman', 'joshua', 'rookie', 'garry',\n",
       "       'donkey', 'huey', 'frederick', 'dudley', 'jedi', 'victor',\n",
       "       'william', 'traitor', 'lordship', 'captains', 'clan', 'rohan',\n",
       "       'robby', 'karl', 'horse', 'rooney', 'jarrett', 'papacy', 'tyrant',\n",
       "       'cavalier', 'sigismund', 'fitz', 'rodney', 'landlord', 'famously',\n",
       "       'hanley', 'emir', 'mike', 'rhinos', 'abdullah', 'mahmud',\n",
       "       'hendrik', 'crowe', 'abel', 'umpires', 'ball', 'mohammad', 'dyer',\n",
       "       'sam', 'mcc', 'friars', 'touchdown', 'rebels', 'bullock', 'envoy',\n",
       "       'xxiii', 'ltc', 'jihad', 'xv', 'xiv', 'quarterbacks', 'bengals',\n",
       "       'midfielder', 'rudolph', 'nickname', 'lucas', 'redman', 'benny',\n",
       "       'throne', 'henrys', 'earl', 'governor', 'vince', 'herd', 'deacon',\n",
       "       'rob', 'frazier', 'astros', 'charlton', 'punt', 'sonny', 'bert',\n",
       "       'manager', 'patriots', 'chuck', 'gott', 'wally', 'otto', 'tigers',\n",
       "       'garrison', 'cossacks', 'grim', 'bob', 'cleric', 'brave', 'osman',\n",
       "       'samuel', 'ivan', 'pete', 'hackett', 'ham', 'warrior', 'warden',\n",
       "       'foley', 'spurs', 'curt', 'wynn', 'marquis', 'hurley', 'haji',\n",
       "       'tarzan', 'yusuf', 'byron', 'milner', 'heinz', 'shaun', 'nawab',\n",
       "       'riff', 'finley', 'rabbis', 'goat', 'wesley', 'disciple', 'gordon',\n",
       "       'moffat', 'maverick', 'axe', 'ord', 'mcmahon', 'howie', 'gregor',\n",
       "       'thomas', 'bucks', 'tyson', 'archdeacon', 'augustus', 'alderman',\n",
       "       'rev', 'boyd', 'frank', 'beckett', 'kuhn', 'edward', 'treason',\n",
       "       'catcher', 'pitt', 'wehrmacht', 'johnnie', 'cossack', 'arnold',\n",
       "       'ben', 'col', 'rudolf', 'successor', 'grandmaster', 'harry',\n",
       "       'loyalist', 'baron', 'keenan', 'trick', 'hawkins', 'cicero',\n",
       "       'ramsey', 'ravens', 'caesars', 'whl', 'defender', 'stephen',\n",
       "       'brigade', 'hat', 'gilbert', 'luck', 'goldsmith', 'guru', 'banjo',\n",
       "       'erwin', 'pugh', 'leonard', 'haq', 'matthias', 'gang', 'mooney',\n",
       "       'mutiny', 'tiger', 'punch', 'khalid', 'bruce', 'ers', 'rule',\n",
       "       'elder', 'looney', 'kin', 'mcmanus'], dtype='<U12')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('male_neighbors', male_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Professions come from Bolukbasi paper \n",
    "\n",
    "https://github.com/tolga-b/debiaswe/blob/master/data/professions.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "professions = np.load('professions.npy')\n",
    "professions_inds = np.load('professions_inds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(professions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate batch produces the index of `batch_size` random professions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate batch data\n",
    "def generate_batch(batch_size):\n",
    "    indicies_p = np.random.choice(professions_inds,1,replace=False)\n",
    "    return indicies_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11787])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicies_p = generate_batch(batch_size)\n",
    "indicies_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'historian'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_model.wv.index2word[2235]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_weights = wiki_model.wv.vectors[:vocab_size,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bolukbasi_debiased_vecs = np.load('bolukbasi_debiased_vecs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_neighbors = np.load('male_neighbors.npy')\n",
    "male_neighbors_inds = np.array([wiki_model.wv.vocab[w].index for w in male_neighbors])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_neighbors = np.load('female_neighbors.npy')\n",
    "female_neighbors_inds = np.array([wiki_model.wv.vocab[w].index for w in female_neighbors])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigbors = np.concatenate([female_neighbors,male_neighbors])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_inds = np.concatenate([female_neighbors_inds,male_neighbors_inds])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    with tf.name_scope('inputs'):\n",
    "        indices_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    with tf.device('/cpu:0'):\n",
    "        with tf.name_scope('embeddings'):\n",
    "            # initialize the embedding weights\n",
    "            embedding = tf.Variable(bolukbasi_debiased_vecs)\n",
    "            # embed profession(s)\n",
    "            embed_inputs = tf.nn.embedding_lookup(embedding,indices_inputs)\n",
    "#             embed_neighbors = tf.nn.embedding_lookup(embedding,neighbors_inds)\n",
    "            embed_neighbors_m = tf.nn.embedding_lookup(embedding,male_neighbors_inds)\n",
    "            embed_neighbors_f = tf.nn.embedding_lookup(embedding,female_neighbors_inds)\n",
    "\n",
    "\n",
    "            # L1 distance\n",
    "#             distances_all = tf.reduce_sum(tf.abs(tf.add(embed_neighbors, tf.negative(embed_inputs))), reduction_indices=1)\n",
    "\n",
    "            distances_m = tf.reduce_sum(tf.abs(tf.add(embed_neighbors_m, tf.negative(embed_inputs))), reduction_indices=1)\n",
    "\n",
    "            distances_f = tf.reduce_sum(tf.abs(tf.add(embed_neighbors_f, tf.negative(embed_inputs))), reduction_indices=1)\n",
    "\n",
    "            # L2 Distance\n",
    "#             distance = tf.sqrt(tf.reduce_sum(tf.squared_difference(neighbors, profession), reduction_indices=1))\n",
    "            \n",
    "            # Get indicies of smallest distances (nearest neighbors)\n",
    "#             _,inds_all = tf.math.top_k(tf.negative(distances_all),k=k_knn)\n",
    "\n",
    "            vals_m,inds_m = tf.math.top_k(tf.negative(distances_m),k=(k_knn//2))\n",
    "\n",
    "            vals_f,inds_f = tf.math.top_k(tf.negative(distances_f),k=(k_knn//2))\n",
    "            \n",
    "            # True if the neighbors are from the 'male' half of the neighbors\n",
    "#             bools = tf.math.greater(inds_all,(len(neighbors_inds)//2))\n",
    "            \n",
    "            # fraction of neighbors that are 'male' biased words\n",
    "#             proportion = tf.reduce_mean(tf.cast(bools, tf.float32))\n",
    "            \n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_sum(tf.abs(vals_m - vals_f))\n",
    "    \n",
    "    tf.summary.scalar('loss', loss)\n",
    "    \n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(0.003).minimize(loss)\n",
    "    \n",
    "#     norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "#     normalized_embeddings = embeddings / norm\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "#     saver = tf.train.Saver()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_p = generate_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8947])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'teenager'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_model.wv.index2word[indices_p[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.4857163\n"
     ]
    }
   ],
   "source": [
    "feed_dict = {indices_inputs: indices_p}\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    res = sess.run(loss, feed_dict)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_steps = 1000001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    \n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "    min_loss = sys.maxsize\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        indices_wv = generate_batch(batch_size)\n",
    "        feed_dict = {indices_inputs: indices_wv}\n",
    "\n",
    "      # Define metadata variable.\n",
    "        run_metadata = tf.RunMetadata()\n",
    "\n",
    "        _, summary, loss_val = session.run([optimizer, merged, loss],\n",
    "                                         feed_dict=feed_dict,\n",
    "                                         run_metadata=run_metadata)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0 and step != 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "        # The average loss is an estimate of the loss over the last 2000\n",
    "        # batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            if average_loss < min_loss:\n",
    "                print('New min loss, saving embedding')\n",
    "                wvs_embed_trained = embedding.eval()\n",
    "\n",
    "                min_loss = average_loss\n",
    "                \n",
    "            average_loss = 0    \n",
    "\n",
    "    # Save the model for checkpoints.\n",
    "    saver.save(session, './model_0')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvs_embed_trained = np.load('wvs_embed_trained_goldberg.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('fasttext_wiki_debias_cluster_wvs_weights', wvs_embed_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvs_embed_trained_norm = np.sqrt(np.sum(np.square(wvs_embed_trained), 1, keepdims=True))\n",
    "wvs_embed_trained_normalized_embeddings = wvs_embed_trained / wvs_embed_trained_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('fasttext_wiki_debias_cluster_wvs', wvs_embed_trained_normalized_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_np_to_bin_model(np_vectors,model_name):\n",
    "    with open(model_name+'.txt', 'w') as we:\n",
    "        we.write('{} {}\\n'.format(vocab_size,vector_dim))\n",
    "        for i in range(vocab_size):\n",
    "            w = wiki_model.wv.index2word[i]\n",
    "            vec = np_vectors[i]\n",
    "            we.write('{} '.format(w))\n",
    "            for v in vec:\n",
    "                we.write(str(v) + ' ')\n",
    "            we.write('\\n')\n",
    "    model = KeyedVectors.load_word2vec_format(model_name+'.txt', binary=False)\n",
    "    model.save_word2vec_format(model_name+'.bin', binary=True)\n",
    "    print('created model '+model_name+'.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created model fasttext_wiki_debias_cluster.bin\n"
     ]
    }
   ],
   "source": [
    "convert_np_to_bin_model(wvs_embed_trained_normalized_embeddings,'fasttext_wiki_debias_cluster')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
