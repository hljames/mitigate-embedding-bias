{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import random\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 22000\n",
    "batch_size = 8\n",
    "eps = 0.0001\n",
    "k=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the original (biased) embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For details of this model, see:\n",
    "\n",
    "https://fasttext.cc/docs/en/english-vectors.html<br>\n",
    "https://arxiv.org/abs/1712.09405<br>\n",
    "https://arxiv.org/abs/1607.01759<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========================--------------------------] 49.1% 470.2/958.4MB downloaded"
     ]
    }
   ],
   "source": [
    "wiki_model = api.load(\"fasttext-wiki-news-subwords-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cvs` refers to the \"context vectors\" or \"output vector\" produced during training -- see \"Distributed Representations of Words and Phrases and their Compositionality\" Mikolov et al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvs = wiki_model.wv.vectors[:vocab_size]\n",
    "cvs = wiki_model.trainables.syn1neg[:vocab_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gender word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_word_pairs_raw = [('he','she'),('man','woman'),('his','her'),('himself','herself'), ('him','her'),('men','women'),('husband','wife'),('boy','girl'),('men','women'),('brother','sister'),('mother','father'),('aunt','uncle'),('grandfather','grandmother'),('son','daughter'),('waiter','waitress'),('niece','nephew'),('girlfriend','boyfriend'),('father-in-law','mother-in-law'),('great uncle','great aunt'),('mr','ms'),('god','goddess'),('stepbrother','stepsister'),('stepmother','stepfather'),('son-in-law','daughter-in-law'),('brother-in-law','sister-in-law'),('landlord','landlady'),('hero','heroine'),('king','queen'),('duke','duchess'),('actor','actress'),('prince','princess'),('host','hostess'),('papa','mama'),('wizard','witch'),('monk','nun')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out words that are not in the first `vocab_size` vectors of the embedding (vectors are sorted by word frequency in the original corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "father-in-law not in vocab\n",
      "great uncle not in vocab\n",
      "stepbrother has index 71120, too large for vocab size 22000\n",
      "stepfather has index 21353, too large for vocab size 22000\n",
      "son-in-law not in vocab\n",
      "brother-in-law not in vocab\n",
      "landlord has index 39383, too large for vocab size 22000\n",
      "host has index 22230, too large for vocab size 22000\n"
     ]
    }
   ],
   "source": [
    "gender_word_pairs = []\n",
    "for gw1,gw2 in gender_word_pairs_raw:\n",
    "    try:\n",
    "        i = wiki_model.wv.vocab[gw1].index\n",
    "    except:\n",
    "        print('{} not in vocab'.format(gw1))\n",
    "        continue;\n",
    "    try:\n",
    "        j = wiki_model.wv.vocab[gw2].index\n",
    "    except:\n",
    "        print('{} not in vocab'.format(gw))\n",
    "        continue;\n",
    "    if j > vocab_size:\n",
    "        print('{} has index {}, too large for vocab size {}'.format(gw1,j,vocab_size))\n",
    "    elif i > vocab_size:\n",
    "        print('{} has index {}, too large for vocab size {}'.format(gw2,j,vocab_size))\n",
    "    else:\n",
    "        gender_word_pairs.append((gw1,gw2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('gender_word_pairs', gender_word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "male, female = zip(*gender_word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_words = list(male)\n",
    "female_words = list(female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_inds = np.array([wiki_model.wv.vocab[w].index for w in male_words])\n",
    "\n",
    "female_inds = np.array([wiki_model.wv.vocab[w].index for w in female_words])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Bolukbasi, Tolga et all \"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\"\n",
    "\n",
    "https://github.com/tolga-b/debiaswe/blob/master/data/gender_specific_full.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "appropriately_gendered_words = [\"he\", \"his\", \"her\", \"she\", \"him\", \"man\", \"women\", \"men\", \"woman\", \"spokesman\", \"wife\", \"himself\", \"son\", \"mother\", \"father\", \"chairman\",\n",
    "\"daughter\", \"husband\", \"guy\", \"girls\", \"girl\", \"boy\", \"boys\", \"brother\", \"spokeswoman\", \"female\", \"sister\", \"male\", \"herself\", \"brothers\", \"dad\",\n",
    "\"actress\", \"mom\", \"sons\", \"girlfriend\", \"daughters\", \"lady\", \"boyfriend\", \"sisters\", \"mothers\", \"king\", \"businessman\", \"grandmother\",\n",
    "\"grandfather\", \"deer\", \"ladies\", \"uncle\", \"males\", \"congressman\", \"grandson\", \"bull\", \"queen\", \"businessmen\", \"wives\", \"widow\",\n",
    "\"nephew\", \"bride\", \"females\", \"aunt\", \"prostate cancer\", \"lesbian\", \"chairwoman\", \"fathers\", \"moms\", \"maiden\", \"granddaughter\",\n",
    "\"younger brother\", \"lads\", \"lion\", \"gentleman\", \"fraternity\", \"bachelor\", \"niece\", \"bulls\", \"husbands\", \"prince\", \"colt\", \"salesman\", \"hers\",\n",
    "\"dude\", \"beard\", \"filly\", \"princess\", \"lesbians\", \"councilman\", \"actresses\", \"gentlemen\", \"stepfather\", \"monks\", \"ex girlfriend\", \"lad\",\n",
    "\"sperm\", \"testosterone\", \"nephews\", \"maid\", \"daddy\", \"mare\", \"fiance\", \"fiancee\", \"kings\", \"dads\", \"waitress\", \"maternal\", \"heroine\",\n",
    "\"nieces\", \"girlfriends\", \"sir\", \"stud\", \"mistress\", \"lions\", \"estranged wife\", \"womb\", \"grandma\", \"maternity\", \"estrogen\", \"ex boyfriend\",\n",
    "\"widows\", \"gelding\", \"diva\", \"teenage girls\", \"nuns\", \"czar\", \"ovarian cancer\", \"countrymen\", \"teenage girl\", \"penis\", \"bloke\", \"nun\",\n",
    "\"brides\", \"housewife\", \"spokesmen\", \"suitors\", \"menopause\", \"monastery\", \"motherhood\", \"brethren\", \"stepmother\", \"prostate\",\n",
    "\"hostess\", \"twin brother\", \"schoolboy\", \"brotherhood\", \"fillies\", \"stepson\", \"congresswoman\", \"uncles\", \"witch\", \"monk\", \"viagra\",\n",
    "\"paternity\", \"suitor\", \"sorority\", \"macho\", \"businesswoman\", \"eldest son\", \"gal\", \"statesman\", \"schoolgirl\", \"fathered\", \"goddess\",\n",
    "\"hubby\", \"stepdaughter\", \"blokes\", \"dudes\", \"strongman\", \"uterus\", \"grandsons\", \"studs\", \"mama\", \"godfather\", \"hens\", \"hen\", \"mommy\",\n",
    "\"estranged husband\", \"elder brother\", \"boyhood\", \"baritone\", \"grandmothers\", \"grandpa\", \"boyfriends\", \"feminism\", \"countryman\",\n",
    "\"stallion\", \"heiress\", \"queens\", \"witches\", \"aunts\", \"semen\", \"fella\", \"granddaughters\", \"chap\", \"widower\", \"salesmen\", \"convent\",\n",
    "\"vagina\", \"beau\", \"beards\", \"handyman\", \"twin sister\", \"maids\", \"gals\", \"housewives\", \"horsemen\", \"obstetrics\", \"fatherhood\",\n",
    "\"councilwoman\", \"princes\", \"matriarch\", \"colts\", \"ma\", \"fraternities\", \"pa\", \"fellas\", \"councilmen\", \"dowry\", \"barbershop\", \"fraternal\",\n",
    "\"ballerina\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_app_gen_indicies(words):\n",
    "    def get_index(w):\n",
    "        try: \n",
    "            return wiki_model.wv.vocab[w].index\n",
    "        except:\n",
    "            print(\"not in model: {}\".format(w))\n",
    "            return vocab_size+10\n",
    "    indices = list(map(lambda word: get_index(word), words))\n",
    "    indices = list(filter(lambda i: i < vocab_size, indices))\n",
    "    return np.flip(np.sort(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not in model: prostate cancer\n",
      "not in model: younger brother\n",
      "not in model: ex girlfriend\n",
      "not in model: estranged wife\n",
      "not in model: ex boyfriend\n",
      "not in model: teenage girls\n",
      "not in model: ovarian cancer\n",
      "not in model: teenage girl\n",
      "not in model: twin brother\n",
      "not in model: eldest son\n",
      "not in model: hubby\n",
      "not in model: estranged husband\n",
      "not in model: elder brother\n",
      "not in model: twin sister\n"
     ]
    }
   ],
   "source": [
    "app_gen_indices = get_indices_in_matrix(appropriately_gendered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     1,     2, ..., 21997, 21998, 21999])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inapp_gen_indices = np.array(list((filter(lambda x: x not in app_gen_indices, np.arange(vocab_size)))))\n",
    "inapp_gen_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per Mikolov's paper \"Distributed Representations of Words and Phrases and their Compositionality,\" we use the following definition of log conditional probability.\n",
    "\n",
    "$$ \\log P(w_O|w_I) \\approx \\log \\sigma ({v'_{wo}}^T v_{wI}) + \\sum_{i=1}^{k} [\\log {\\sigma ({{-v'_{wi}}^T v_{wI}})}] $$\n",
    "\n",
    "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per the paper, we use the unigram distribution raised to the 3/4th power from which to draw samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = np.array([wiki_model.wv.vocab[wiki_model.wv.index2word[i]].count for i in range(vocab_size)])\n",
    "word_counts_power = np.power(word_counts,.75)\n",
    "sampling_dist = np.true_divide(word_counts_power,np.sum(word_counts_power))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate batch produces indicies of words to debias and indicies of the k samples with which to calculate the probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate batch data\n",
    "def generate_batch(batch_size):\n",
    "    indicies_wv = np.random.choice(inapp_gen_indices,batch_size,replace=False)\n",
    "    sampling_dist_inapp_gen_indices = sampling_dist[inapp_gen_indices] / np.sum(sampling_dist[inapp_gen_indices])\n",
    "    samples = np.random.choice(inapp_gen_indices,size=k,replace=False,p=sampling_dist_inapp_gen_indices)\n",
    "    samples_2d = np.array([samples for i in range(batch_size)])\n",
    "    return indicies_wv, samples_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_wv, samples_2d = generate_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_weights = wiki_model.wv.vectors[:vocab_size,:]\n",
    "original_cv_weights = cvs[:vocab_size,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    with tf.name_scope('inputs'):\n",
    "        indices_samples = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        samples_inputs = tf.placeholder(tf.int32, shape=[batch_size,k])\n",
    "    with tf.device('/cpu:0'):\n",
    "        with tf.name_scope('embeddings'):\n",
    "            # initialize the the biased embedding weights\n",
    "            embeddings_wv = tf.Variable(wvs)\n",
    "            embeddings_cv = tf.Variable(cvs)\n",
    "        # calculate log P(w_i|m_j) for all w_i in indices_samples and all m_j in male_words\n",
    "        embed_wv = tf.nn.embedding_lookup(embeddings_wv,indices_samples)\n",
    "        embed_cv_he = tf.nn.embedding_lookup(embeddings_wv,male_inds)\n",
    "        embed_sv = tf.nn.embedding_lookup(embeddings_wv,samples_inputs)\n",
    "        prod_he = tf.matmul(embed_wv, tf.transpose(embed_cv_he))\n",
    "        first_term_he = tf.math.log_sigmoid(prod_he)\n",
    "        embed_sv = tf.reshape(embed_sv,[vector_dim,batch_size,k])\n",
    "        prod_2_he = tf.tensordot(embed_cv_he, -1*embed_sv, axes=[[1],[0]])\n",
    "        prod_2_log_sig_he = tf.math.log_sigmoid(prod_2_he)\n",
    "        second_term_he = tf.reduce_sum(prod_2_log_sig_he,2)\n",
    "        prob_he = first_term_he + tf.transpose(second_term_he)\n",
    "        \n",
    "        # calculate log P(w_i|f_j) for all w_i in indices_samples and all m_j in female_words\n",
    "        embed_cv_she = tf.nn.embedding_lookup(embeddings_wv,female_inds)\n",
    "        prod_she = tf.matmul(embed_wv, tf.transpose(embed_cv_she))\n",
    "        first_term_she = tf.math.log_sigmoid(prod_she)\n",
    "        prod_2_she = tf.tensordot(embed_cv_she, -1*embed_sv, axes=[[1],[0]])\n",
    "        prod_2_log_sig_she = tf.math.log_sigmoid(prod_2_she)\n",
    "        second_term_she = tf.reduce_sum(prod_2_log_sig_she,2)\n",
    "        prob_she = first_term_she + tf.transpose(second_term_she)\n",
    "\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_sum(tf.abs(prob_he - prob_she))\n",
    "    \n",
    "    tf.summary.scalar('loss', loss)\n",
    "    \n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(0.003).minimize(loss)\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test `generate_batch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_wv, samples = generate_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105905.45\n"
     ]
    }
   ],
   "source": [
    "feed_dict = {indices_samples: indices_wv, samples_inputs : samples}\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    res = sess.run(loss, feed_dict)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "('Average loss at step ', 0, ': ', 145808.46875)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 2000, ': ', 42407.46003369141)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 4000, ': ', 5409.074129760742)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 6000, ': ', 3871.2420838012695)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 8000, ': ', 3066.7472525939943)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 10000, ': ', 2479.593877166748)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 12000, ': ', 2069.0306828308107)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 14000, ': ', 1737.6057858886718)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 16000, ': ', 1505.0508660736084)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 18000, ': ', 1315.2425707702637)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 20000, ': ', 1191.3702390136718)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 22000, ': ', 1047.7088432922362)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 24000, ': ', 994.6787582092285)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 26000, ': ', 934.8502577438354)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 28000, ': ', 860.5945480957031)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 30000, ': ', 792.9371610107422)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 32000, ': ', 754.5787986755371)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 34000, ': ', 711.569315574646)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 36000, ': ', 695.263195526123)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 38000, ': ', 645.1027502441407)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 40000, ': ', 629.826758605957)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 42000, ': ', 608.93626146698)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 44000, ': ', 601.5832300262451)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 46000, ': ', 562.487641494751)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 48000, ': ', 556.8718239898682)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 50000, ': ', 540.2575562667847)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 52000, ': ', 528.7341520385742)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 54000, ': ', 508.58222772216794)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 56000, ': ', 503.1282885971069)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 58000, ': ', 491.2588805236816)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 60000, ': ', 483.05298054504397)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 62000, ': ', 472.98528567504883)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 64000, ': ', 463.962200012207)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 66000, ': ', 442.9358182220459)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 68000, ': ', 446.4171011581421)\n",
      "('Average loss at step ', 70000, ': ', 436.58444353485106)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 72000, ': ', 427.57048745727536)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 74000, ': ', 426.17315016174314)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 76000, ': ', 412.91728693389894)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 78000, ': ', 415.3095647964478)\n",
      "('Average loss at step ', 80000, ': ', 405.3442787322998)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 82000, ': ', 404.53129716491696)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 84000, ': ', 392.0806543121338)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 86000, ': ', 389.1293300476074)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 88000, ': ', 380.4458424224853)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 90000, ': ', 377.4576517486572)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 92000, ': ', 374.8427999343872)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 94000, ': ', 365.23044997406004)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 96000, ': ', 363.7244051437378)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 98000, ': ', 360.2349257888794)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 100000, ': ', 354.2298588943481)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 102000, ': ', 351.262486618042)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 104000, ': ', 347.00890767669677)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 106000, ': ', 343.8736577224731)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 108000, ': ', 339.0336686782837)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 110000, ': ', 339.02845637512206)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 112000, ': ', 329.53523623657225)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 114000, ': ', 329.3953263778686)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 116000, ': ', 328.17622216033936)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 118000, ': ', 321.26265446472166)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 120000, ': ', 319.51121015930175)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 122000, ': ', 317.5609171142578)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 124000, ': ', 309.5405583343506)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 126000, ': ', 308.2027518386841)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 128000, ': ', 305.0422484512329)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 130000, ': ', 303.7968448791504)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 132000, ': ', 297.0159010467529)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 134000, ': ', 300.5370947341919)\n",
      "('Average loss at step ', 136000, ': ', 296.2286835861206)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 138000, ': ', 295.5436675033569)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 140000, ': ', 292.3655189590454)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 142000, ': ', 285.83236563873294)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 144000, ': ', 282.7929710006714)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 146000, ': ', 284.45904916381835)\n",
      "('Average loss at step ', 148000, ': ', 278.94659915924075)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 150000, ': ', 277.10903553009035)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 152000, ': ', 272.2655927047729)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 154000, ': ', 275.68386693572995)\n",
      "('Average loss at step ', 156000, ': ', 267.53957084655764)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 158000, ': ', 269.4244949951172)\n",
      "('Average loss at step ', 160000, ': ', 264.8837320480347)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 162000, ': ', 263.6691943740845)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 164000, ': ', 259.28333135223386)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 166000, ': ', 256.29738145446777)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 168000, ': ', 258.56338063812257)\n",
      "('Average loss at step ', 170000, ': ', 255.0818938522339)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 172000, ': ', 252.76811903381346)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 174000, ': ', 247.61823210906982)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 176000, ': ', 248.7605779876709)\n",
      "('Average loss at step ', 178000, ': ', 248.5592027130127)\n",
      "('Average loss at step ', 180000, ': ', 246.14815787506103)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 182000, ': ', 244.02170764160155)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 184000, ': ', 241.0154856185913)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 186000, ': ', 241.24791509246828)\n",
      "('Average loss at step ', 188000, ': ', 240.0489268798828)\n",
      "New min loss, saving embedding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average loss at step ', 190000, ': ', 237.15282377624513)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 192000, ': ', 235.380104927063)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 194000, ': ', 234.71038902282714)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 196000, ': ', 230.02637878417968)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 198000, ': ', 229.08726819610595)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 200000, ': ', 226.67460459136962)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 202000, ': ', 225.1324543838501)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 204000, ': ', 223.38908647155762)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 206000, ': ', 223.0223366241455)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 208000, ': ', 221.05493197631836)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 210000, ': ', 217.70255585479737)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 212000, ': ', 221.12319750976562)\n",
      "('Average loss at step ', 214000, ': ', 218.85708378601075)\n",
      "('Average loss at step ', 216000, ': ', 215.98708472442627)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 218000, ': ', 214.477897895813)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 220000, ': ', 213.79629985046387)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 222000, ': ', 211.3708242263794)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 224000, ': ', 210.65302658081055)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 226000, ': ', 209.68742599487305)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 228000, ': ', 207.54734174346925)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 230000, ': ', 205.826068901062)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 232000, ': ', 203.10220947647096)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 234000, ': ', 203.22781204223634)\n",
      "('Average loss at step ', 236000, ': ', 201.37645377349853)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 238000, ': ', 200.22522101211547)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 240000, ': ', 197.01686785125733)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 242000, ': ', 196.1745241394043)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 244000, ': ', 194.92309911727907)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 246000, ': ', 195.02963592910766)\n",
      "('Average loss at step ', 248000, ': ', 194.60423638534547)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 250000, ': ', 191.051122795105)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 252000, ': ', 192.04407421875)\n",
      "('Average loss at step ', 254000, ': ', 189.42091886520384)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 256000, ': ', 189.1909671897888)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 258000, ': ', 189.006906539917)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 260000, ': ', 187.33296251678468)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 262000, ': ', 188.33174603271485)\n",
      "('Average loss at step ', 264000, ': ', 184.35966193008423)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 266000, ': ', 184.90307510375976)\n",
      "('Average loss at step ', 268000, ': ', 184.4679323348999)\n",
      "('Average loss at step ', 270000, ': ', 180.3561545677185)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 272000, ': ', 180.518223777771)\n",
      "('Average loss at step ', 274000, ': ', 179.4529274482727)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 276000, ': ', 177.69515420913697)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 278000, ': ', 176.1340815963745)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 280000, ': ', 174.78579087066652)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 282000, ': ', 176.70673620986938)\n",
      "('Average loss at step ', 284000, ': ', 174.31964939117432)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 286000, ': ', 173.34425982284546)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 288000, ': ', 174.2426353187561)\n",
      "('Average loss at step ', 290000, ': ', 168.32401582336425)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 292000, ': ', 170.95818262481689)\n",
      "('Average loss at step ', 294000, ': ', 167.60114651489258)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 296000, ': ', 168.42651651763916)\n",
      "('Average loss at step ', 298000, ': ', 164.06931045913697)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 300000, ': ', 165.5398332939148)\n",
      "('Average loss at step ', 302000, ': ', 163.89514641952513)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 304000, ': ', 162.73142957305907)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 306000, ': ', 164.05855787658692)\n",
      "('Average loss at step ', 308000, ': ', 162.65840396881103)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 310000, ': ', 161.8378684234619)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 312000, ': ', 160.05441650009155)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 314000, ': ', 158.8790364189148)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 316000, ': ', 158.0603483772278)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 318000, ': ', 158.90760372543335)\n",
      "('Average loss at step ', 320000, ': ', 156.1690315322876)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 322000, ': ', 155.41762270736695)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 324000, ': ', 157.03995074081422)\n",
      "('Average loss at step ', 326000, ': ', 155.3457547492981)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 328000, ': ', 152.4300377960205)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 330000, ': ', 152.68199880599977)\n",
      "('Average loss at step ', 332000, ': ', 151.50903995895385)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 334000, ': ', 150.55403025436402)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 336000, ': ', 152.44915704727174)\n",
      "('Average loss at step ', 338000, ': ', 148.25364426422118)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 340000, ': ', 147.5369897994995)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 342000, ': ', 148.12698482131958)\n",
      "('Average loss at step ', 344000, ': ', 147.0457451057434)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 346000, ': ', 148.18238869857788)\n",
      "('Average loss at step ', 348000, ': ', 146.1864743537903)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 350000, ': ', 143.40780393981933)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 352000, ': ', 142.30851718902588)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 354000, ': ', 143.1905089378357)\n",
      "('Average loss at step ', 356000, ': ', 140.05754274749756)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 358000, ': ', 141.145471118927)\n",
      "('Average loss at step ', 360000, ': ', 139.35837712860106)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 362000, ': ', 138.48995405960082)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 364000, ': ', 139.14452939987183)\n",
      "('Average loss at step ', 366000, ': ', 138.17370714950562)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 368000, ': ', 137.6683822517395)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 370000, ': ', 136.37486028289794)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 372000, ': ', 135.02446714782715)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 374000, ': ', 136.49835729599)\n",
      "('Average loss at step ', 376000, ': ', 137.12093442153932)\n",
      "('Average loss at step ', 378000, ': ', 132.7455958633423)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 380000, ': ', 132.45365290832518)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 382000, ': ', 129.2339816017151)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 384000, ': ', 131.06598671340942)\n",
      "('Average loss at step ', 386000, ': ', 128.5761298828125)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 388000, ': ', 130.7579765625)\n",
      "('Average loss at step ', 390000, ': ', 129.32978395462035)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average loss at step ', 392000, ': ', 129.4457576522827)\n",
      "('Average loss at step ', 394000, ': ', 125.52518164825439)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 396000, ': ', 129.25232378387452)\n",
      "('Average loss at step ', 398000, ': ', 128.47520518493653)\n",
      "('Average loss at step ', 400000, ': ', 124.71199565505981)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 402000, ': ', 125.79988800811768)\n",
      "('Average loss at step ', 404000, ': ', 127.81772748565673)\n",
      "('Average loss at step ', 406000, ': ', 122.22835911941529)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 408000, ': ', 123.12983395385743)\n",
      "('Average loss at step ', 410000, ': ', 124.14998905181885)\n",
      "('Average loss at step ', 412000, ': ', 124.14651356124878)\n",
      "('Average loss at step ', 414000, ': ', 124.02596559143066)\n",
      "('Average loss at step ', 416000, ': ', 122.56485943984985)\n",
      "('Average loss at step ', 418000, ': ', 120.54446464538574)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 420000, ': ', 118.62080647277833)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 422000, ': ', 119.45140591430665)\n",
      "('Average loss at step ', 424000, ': ', 117.81439678955078)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 426000, ': ', 118.11462162780762)\n",
      "('Average loss at step ', 428000, ': ', 119.01498611450195)\n",
      "('Average loss at step ', 430000, ': ', 118.00006645584106)\n",
      "('Average loss at step ', 432000, ': ', 117.73903930664062)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 434000, ': ', 115.17023691558838)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 436000, ': ', 115.48965344619751)\n",
      "('Average loss at step ', 438000, ': ', 115.36782316207886)\n",
      "('Average loss at step ', 440000, ': ', 116.84408729171753)\n",
      "('Average loss at step ', 442000, ': ', 113.95377696228027)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 444000, ': ', 112.02197034072876)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 446000, ': ', 115.34907470703125)\n",
      "('Average loss at step ', 448000, ': ', 111.80665700912476)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 450000, ': ', 112.2235195350647)\n",
      "('Average loss at step ', 452000, ': ', 112.6832615737915)\n",
      "('Average loss at step ', 454000, ': ', 110.01406506347656)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 456000, ': ', 110.14770472335816)\n",
      "('Average loss at step ', 458000, ': ', 109.07007488250733)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 460000, ': ', 109.67220985794067)\n",
      "('Average loss at step ', 462000, ': ', 109.09784705734253)\n",
      "('Average loss at step ', 464000, ': ', 108.82070112228394)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 466000, ': ', 107.62813530731201)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 468000, ': ', 108.89281154632569)\n",
      "('Average loss at step ', 470000, ': ', 105.25341334915161)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 472000, ': ', 107.56815030670165)\n",
      "('Average loss at step ', 474000, ': ', 105.56510283660889)\n",
      "('Average loss at step ', 476000, ': ', 105.10479559326171)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 478000, ': ', 104.65240327644348)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 480000, ': ', 102.45883382034302)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 482000, ': ', 104.32078618812561)\n",
      "('Average loss at step ', 484000, ': ', 101.9045786113739)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 486000, ': ', 100.12071435165406)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 488000, ': ', 101.88035942077637)\n",
      "('Average loss at step ', 490000, ': ', 101.50097294998169)\n",
      "('Average loss at step ', 492000, ': ', 101.69145427322388)\n",
      "('Average loss at step ', 494000, ': ', 100.32546558761597)\n",
      "('Average loss at step ', 496000, ': ', 97.58649814605712)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 498000, ': ', 101.23792844200135)\n",
      "('Average loss at step ', 500000, ': ', 98.93159872817994)\n",
      "('Average loss at step ', 502000, ': ', 97.8501688632965)\n",
      "('Average loss at step ', 504000, ': ', 98.78417957305908)\n",
      "('Average loss at step ', 506000, ': ', 95.79108491516114)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 508000, ': ', 96.51987373161316)\n",
      "('Average loss at step ', 510000, ': ', 94.77739554977417)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 512000, ': ', 95.54049204444885)\n",
      "('Average loss at step ', 514000, ': ', 94.56901314926148)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 516000, ': ', 96.55765511512756)\n",
      "('Average loss at step ', 518000, ': ', 95.2826289176941)\n",
      "('Average loss at step ', 520000, ': ', 94.02040725326538)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 522000, ': ', 94.74210389709472)\n",
      "('Average loss at step ', 524000, ': ', 92.60957133293152)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 526000, ': ', 92.11696273612976)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 528000, ': ', 90.25889332771301)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 530000, ': ', 88.8489325389862)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 532000, ': ', 91.26323350906372)\n",
      "('Average loss at step ', 534000, ': ', 90.23352535629273)\n",
      "('Average loss at step ', 536000, ': ', 89.28489712142944)\n",
      "('Average loss at step ', 538000, ': ', 87.50155859565734)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 540000, ': ', 88.7414430732727)\n",
      "('Average loss at step ', 542000, ': ', 87.3192788619995)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 544000, ': ', 87.30602456283569)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 546000, ': ', 88.7937198638916)\n",
      "('Average loss at step ', 548000, ': ', 85.10571862792969)\n",
      "New min loss, saving embedding\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-1fec0102cc3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         _, summary, loss_val = session.run([optimizer, merged, loss],\n\u001b[1;32m     21\u001b[0m                                          \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                                          run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0maverage_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haileyjames/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haileyjames/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haileyjames/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haileyjames/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haileyjames/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haileyjames/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 1000001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "    min_loss = sys.maxsize\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        indices_wv, samples = generate_batch(batch_size)\n",
    "        feed_dict = {indices_samples: indices_wv, samples_inputs : samples}\n",
    "\n",
    "\n",
    "\n",
    "      # Define metadata variable.\n",
    "        run_metadata = tf.RunMetadata()\n",
    "\n",
    "        _, summary, loss_val = session.run([optimizer, merged, loss],\n",
    "                                         feed_dict=feed_dict,\n",
    "                                         run_metadata=run_metadata)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "        # The average loss is an estimate of the loss over the last 2000\n",
    "        # batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            if average_loss < min_loss:\n",
    "                print('New min loss, saving embedding')\n",
    "                wvs_embed_trained = embeddings_wv.eval()\n",
    "\n",
    "                cvs_embed_trained = embeddings_cv.eval()\n",
    "\n",
    "                min_loss = average_loss\n",
    "                \n",
    "            average_loss = 0    \n",
    "            \n",
    "    # Save the model for checkpoints.\n",
    "    saver.save(session, './model_0')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('fasttext_wiki_debias_prob_wvs_weights', wvs_embed_trained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('fasttext_wiki_debias_prob_cvs_weights', cvs_embed_trained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvs_embed_trained_norm = np.sqrt(np.sum(np.square(wvs_embed_trained), 1, keepdims=True))\n",
    "wvs_embed_trained_normalized_embeddings = wvs_embed_trained / wvs_embed_trained_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('fasttext_wiki_debias_prob_wvs', wvs_embed_trained_normalized_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model as .bin to faciliate comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_np_to_bin_model(np_vectors,model_name):\n",
    "    with open(model_name+'.txt', 'w') as we:\n",
    "        we.write('{} {}\\n'.format(vocab_size,vector_dim))\n",
    "        for i in range(vocab_size):\n",
    "            w = wiki_model.wv.index2word[i]\n",
    "            vec = np_vectors[i]\n",
    "            we.write('{} '.format(w))\n",
    "            for v in vec:\n",
    "                we.write(str(v) + ' ')\n",
    "            we.write('\\n')\n",
    "    model = KeyedVectors.load_word2vec_format(model_name+'.txt', binary=False)\n",
    "    model.save_word2vec_format(model_name+'.bin', binary=True)\n",
    "    print('created model '+model_name+'.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created model fasttext_wiki_debias_prob.bin\n"
     ]
    }
   ],
   "source": [
    "convert_np_to_bin_model(wvs_embed_trained_normalized_embeddings,'fasttext_wiki_debias_prob')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
