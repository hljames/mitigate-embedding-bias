{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Dense, Input, Reshape, Flatten, dot, Add\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import itertools\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "debiased_probs = np.load('debiased_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(debiased_probs)\n",
    "# vocab_size = 22000\n",
    "vector_dim = 100\n",
    "batch_size = 32\n",
    "eps = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate batch data\n",
    "def generate_batch(batch_size):\n",
    "    indices = np.random.choice(vocab_size,batch_size,replace=False)\n",
    "    real_prods = debiased_probs[np.ix_(indices,indices)]\n",
    "    return indices, real_prods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate that this method of generate the batches works as expected\n",
    "\n",
    "X_train: random word indices for which we will update the weights/vectors\n",
    "\n",
    "Y_train: the expected normalized dot product between the vectors (see create_debiased_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.random.choice(10,5,replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 0, 2, 5, 6])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
       "       [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       "       [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
       "       [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n",
       "       [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n",
       "       [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n",
       "       [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n",
       "       [70, 71, 72, 73, 74, 75, 76, 77, 78, 79],\n",
       "       [80, 81, 82, 83, 84, 85, 86, 87, 88, 89],\n",
       "       [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = np.arange(100).reshape(10,10)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[99, 90, 92, 95, 96],\n",
       "       [ 9,  0,  2,  5,  6],\n",
       "       [29, 20, 22, 25, 26],\n",
       "       [59, 50, 52, 55, 56],\n",
       "       [69, 60, 62, 65, 66]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = probs[np.ix_(x_train,x_train)]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify approximation of matrix values using negative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to be safe, we'll use a words for which the probabilities should not have been altered (appropriately genedered words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_i = wiki_model.wv.vocab['prince'].index\n",
    "p_j = wiki_model.wv.vocab['queen'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "822"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1002"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.312238735049531e-06"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debiased_probs[p_i,p_j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-11.584181029948848"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(debiased_probs[p_i,p_j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above will be the value that we're aiming for. From the paper: \"We define Negative sampling\n",
    "(NEG) by the objective ... (see equation) which is used to replace every log P(wO|wI ) term in the Skip-gram objective\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative sampling approximation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_vec = wiki_model.wv.vectors[p_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_vec = wiki_model.wv.vectors[p_j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1./(1.+(np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605.4208"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(i_vec,j_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_term = np.log(sigmoid(np.dot(i_vec,j_vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the paper, \"Our experiments indicate that values\n",
    "of k in the range 5–20 are useful for small training datasets, while for large datasets the k can be as\n",
    "small as 2–5.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.210340371976182\n",
      "-9.21033786527982\n",
      "-8.364109242160499\n",
      "-0.14256124519539537\n",
      "-9.210340371976182\n",
      "9.99950002835643e-05\n",
      "9.999500033329732e-05\n",
      "9.999496631480276e-05\n",
      "-9.210340371976182\n",
      "9.999500033041103e-05\n",
      "-0.01238183780225174\n",
      "-9.210340371976182\n",
      "-9.13753330352398\n",
      "6.223938614676579e-05\n",
      "-9.210340371976182\n",
      "-9.210340371976182\n",
      "-8.254129579052506\n",
      "-9.210340371976182\n",
      "-9.210340371975958\n",
      "-9.210340226760936\n"
     ]
    }
   ],
   "source": [
    "second_term = 0.\n",
    "k=20\n",
    "while k > 0:\n",
    "    r_i = np.random.randint(vocab_size)\n",
    "    if r_i != p_i:\n",
    "        r_vec = wiki_model.wv.vectors[r_i]\n",
    "        new_val = np.log(sigmoid(np.dot((-1*r_vec),j_vec)) + eps)\n",
    "        print(new_val)\n",
    "        second_term += new_val\n",
    "        k -= 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-118.01365405623122"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-118.01365405623122"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_term + second_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Not a good approximation!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_model = Word2Vec.load(\"english-wikipedia-articles-20170820-models/enwiki_2017_08_20_fasttext.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_weights = wiki_model.wv.vectors[:vocab_size,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22000, 100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    with tf.name_scope('inputs'):\n",
    "        x_train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        y_train_inputs = tf.placeholder(tf.float32, shape=[batch_size,batch_size])\n",
    "    with tf.device('/cpu:0'):\n",
    "        with tf.name_scope('embeddings'):\n",
    "            # initialize the the biased embedding weights\n",
    "            embeddings = tf.Variable(original_weights)\n",
    "        embed = tf.nn.embedding_lookup(embeddings,x_train_inputs)\n",
    "        dot_prod = tf.tensordot(embed,tf.transpose(embed),1)\n",
    "        dot_prod_norm = tf.sqrt(tf.reduce_sum(tf.square(dot_prod), 1, keepdims=True))\n",
    "        dot_prod_normed = dot_prod / dot_prod_norm\n",
    "        pred_vals = tf.log(tf.sigmoid(dot_prod_normed) + eps)\n",
    "        indices = np.arange(batch_size)\n",
    "        for i in indices:\n",
    "            for j in indices:\n",
    "                indices_tensor = tf.convert_to_tensor([[m,j] for m in np.where(indices!=i)[0]])\n",
    "#                 print('dot_prod shape: {}'.format(dot_prod.get_shape().as_list()))\n",
    "#                 print('indices_tensor shape: {}'.format(indices_tensor.get_shape().as_list()))\n",
    "                neg_vals = tf.gather_nd(dot_prod_normed,indices_tensor,name='gather_neg_indices')\n",
    "                neg_vals_flipped = -1*neg_vals\n",
    "                neg_sig = tf.sigmoid(neg_vals_flipped)\n",
    "                neg_samples = tf.log(neg_sig + eps)\n",
    "#                 print('neg_samples shape: {}'.format(neg_samples))\n",
    "                neg_samples_sum = tf.reduce_sum(neg_samples,keepdims=True)\n",
    "#                 print('neg_samples_sum shape: {}'.format(neg_samples_sum))\n",
    "#                 print('neg_samples_sum type: {}'.format(type(neg_samples_sum)))\n",
    "                delta = tf.SparseTensor([[i,j]],neg_samples_sum,[batch_size,batch_size])\n",
    "                pred_vals = pred_vals + tf.sparse_tensor_to_dense(delta)\n",
    "    with tf.name_scope('loss'):\n",
    "        log_val = tf.log(y_train_inputs + eps)\n",
    "        loss = tf.reduce_sum(tf.abs(log_val - pred_vals))\n",
    "    \n",
    "    tf.summary.scalar('loss', loss)\n",
    "    \n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "    \n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt, yt = generate_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151823.03\n"
     ]
    }
   ],
   "source": [
    "feed_dict = {x_train_inputs: xt, y_train_inputs: yt}\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    res = sess.run(loss, feed_dict)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note the exceptionally high loss before training...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "('Average loss at step ', 0, ': ', 14717.12109375)\n",
      "('Average loss at step ', 2000, ': ', 14841.26538330078)\n",
      "('Average loss at step ', 4000, ': ', 14850.408685058594)\n",
      "('Average loss at step ', 6000, ': ', 14862.140109863281)\n",
      "('Average loss at step ', 8000, ': ', 14877.74851123047)\n",
      "('Average loss at step ', 10000, ': ', 14889.898091308594)\n",
      "('Average loss at step ', 12000, ': ', 14896.11196875)\n",
      "('Average loss at step ', 14000, ': ', 14908.872233398437)\n",
      "('Average loss at step ', 16000, ': ', 14923.580012695313)\n",
      "('Average loss at step ', 18000, ': ', 14936.589776367187)\n",
      "('Average loss at step ', 20000, ': ', 14942.658600585937)\n",
      "('Average loss at step ', 22000, ': ', 14961.238532714844)\n",
      "('Average loss at step ', 24000, ': ', 14969.888884765625)\n",
      "('Average loss at step ', 26000, ': ', 14978.608094726562)\n",
      "('Average loss at step ', 28000, ': ', 14996.662852539062)\n",
      "('Average loss at step ', 30000, ': ', 15009.816293457032)\n",
      "('Average loss at step ', 32000, ': ', 15015.808973632813)\n",
      "('Average loss at step ', 34000, ': ', 15031.206918457032)\n",
      "('Average loss at step ', 36000, ': ', 15039.682741699218)\n",
      "('Average loss at step ', 38000, ': ', 15055.932418945313)\n",
      "('Average loss at step ', 40000, ': ', 15058.964115234376)\n",
      "('Average loss at step ', 42000, ': ', 15073.450531738281)\n",
      "('Average loss at step ', 44000, ': ', 15081.2737890625)\n",
      "('Average loss at step ', 46000, ': ', 15100.17171142578)\n",
      "('Average loss at step ', 48000, ': ', 15112.00367578125)\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "# Open a writer to write summaries.\n",
    "\n",
    "# We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size)\n",
    "        feed_dict = {x_train_inputs: batch_inputs, y_train_inputs: batch_labels}\n",
    "\n",
    "      # Define metadata variable.\n",
    "        run_metadata = tf.RunMetadata()\n",
    "\n",
    "        _, summary, loss_val = session.run([optimizer, merged, loss],\n",
    "                                         feed_dict=feed_dict,\n",
    "                                         run_metadata=run_metadata)\n",
    "        average_loss += loss_val\n",
    "\n",
    "      # Add returned summaries to writer in each step.\n",
    "      # Add metadata to visualize the graph for the last run.\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "        # The average loss is an estimate of the loss over the last 2000\n",
    "        # batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "    # Save the model for checkpoints.\n",
    "    saver.save(session, os.path.join(log_dir, 'model.ckpt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
