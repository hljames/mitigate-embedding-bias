{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Dense, Input, Reshape, Flatten, dot, Add\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "debiased_mat_alt = np.load('debiased_matrix_alt_orig.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(debiased_mat_alt_2)\n",
    "# vocab_size = 22000\n",
    "vector_dim = 100\n",
    "batch_size = 8\n",
    "eps = 0.0001\n",
    "k=20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_model = Word2Vec.load(\"english-wikipedia-articles-20170820-models/enwiki_2017_08_20_fasttext.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_word_pairs = [('he','she'),('man','woman'),('his','her'),('himself','herself'), ('him','her'),('men','women'),('husband','wife'),('girl','boy'),('men','women'),('brother','sister'),('mother','father'),('aunt','uncle'),('grandfather','grandmother'),('son','daughter'),('waiter','waitress'),('niece','nephew')]\n",
    "gender_word_pairs_simple = [w for p in gender_word_pairs for w in p] \n",
    "gw_dict = {w:i for (i,w) in enumerate(gender_word_pairs_simple)}\n",
    "\n",
    "gender_inds = np.array([wiki_model.wv.vocab[w].index for w in gender_word_pairs_simple])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   13,    42,   276,   789,    17,    40,   551,  2127,    94,\n",
       "          40,   402,   435,  1250,   526,  1020,  1218,   402,   435,\n",
       "         627,  1077,   591,   386,  6642,  3005,  4021,  7034,   306,\n",
       "         629, 21467, 20573, 10387,  5092])"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aunt': 22,\n",
       " 'boy': 15,\n",
       " 'brother': 18,\n",
       " 'daughter': 27,\n",
       " 'father': 21,\n",
       " 'girl': 14,\n",
       " 'grandfather': 24,\n",
       " 'grandmother': 25,\n",
       " 'he': 0,\n",
       " 'her': 9,\n",
       " 'herself': 7,\n",
       " 'him': 8,\n",
       " 'himself': 6,\n",
       " 'his': 4,\n",
       " 'husband': 12,\n",
       " 'man': 2,\n",
       " 'men': 16,\n",
       " 'mother': 20,\n",
       " 'nephew': 31,\n",
       " 'niece': 30,\n",
       " 'she': 1,\n",
       " 'sister': 19,\n",
       " 'son': 26,\n",
       " 'uncle': 23,\n",
       " 'waiter': 28,\n",
       " 'waitress': 29,\n",
       " 'wife': 13,\n",
       " 'woman': 3,\n",
       " 'women': 17}"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gw_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = np.array([wiki_model.wv.vocab[wiki_model.wv.index2word[i]].count for i in range(vocab_size)])\n",
    "word_counts_power = np.power(word_counts,.75)\n",
    "sampling_dist = np.true_divide(word_counts_power,np.sum(word_counts_power))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.e ** -x)\n",
    "\n",
    "wvs = np.load('english-wikipedia-articles-20170820-models/enwiki_2017_08_20_fasttext.model.wv.vectors.npy')\n",
    "wvs = wvs[:vocab_size]\n",
    "\n",
    "cvs = np.load('english-wikipedia-articles-20170820-models/enwiki_2017_08_20_fasttext.model.trainables.syn1neg.npy')\n",
    "cvs = cvs[:vocab_size]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate batch data\n",
    "def generate_batch(batch_size):\n",
    "    indices_wv = np.random.choice(vocab_size,batch_size,replace=False)\n",
    "    indices_cv = np.random.choice(vocab_size,batch_size,replace=False)\n",
    "    y_probs = np.full((batch_size,batch_size),-1.)\n",
    "    samples = np.random.choice(vocab_size,size=k,replace=False,p=sampling_dist)\n",
    "    samples_2d = np.array([samples for i in range(batch_size)])\n",
    "    for i,ind_i in enumerate(indices_wv):\n",
    "        for j, ind_j in enumerate(indices_cv):\n",
    "            if ind_j in gender_inds:\n",
    "#                 ind_gw = gw_dict[wiki_model.wv.index2word[ind_j]]\n",
    "#                 y_probs[i,j] = debiased_mat_alt[ind_i,ind_gw]\n",
    "                y_probs[i,j] = debiased_mat_alt_2[ind_i,ind_j]\n",
    "\n",
    "            else:\n",
    "                v_prime_wo = wvs[ind_i]\n",
    "                v_wi = cvs[ind_j]\n",
    "                first_term = np.log(sigmoid((np.matmul(v_prime_wo,v_wi))))\n",
    "                second_term = np.sum([np.log(sigmoid(-1*np.matmul(wvs[s],v_wi))) for s in samples])\n",
    "                y_probs[i,j] = np.exp(first_term + second_term)\n",
    "    return indices_wv, indices_cv, samples_2d, y_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_wv, indices_cv, samples_2d, yt = generate_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_weights = wiki_model.wv.vectors[:vocab_size,:]\n",
    "original_cv_weights = cvs[:vocab_size,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_wv_weights = np.load('wvs_embed_trained.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_cv_weights = np.load('cvs_embed_trained.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per Mikolov's paper \"Distributed Representations of Words and Phrases and their Compositionality,\" we use the following definition of each log conditional probability.\n",
    "\n",
    "$$ \\log P(w_O|w_I) \\approx \\log \\sigma ({v'_{wo}}^T v_{wI}) + \\sum_{i=1}^{k} [\\log {\\sigma ({{-v'_{wi}}^T v_{wI}})}] $$\n",
    "\n",
    "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    with tf.name_scope('inputs'):\n",
    "        indices_wv_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        indices_cv_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        samples_inputs = tf.placeholder(tf.int32, shape=[batch_size,k])\n",
    "        y_train_inputs = tf.placeholder(tf.float32, shape=[batch_size,batch_size])\n",
    "    with tf.device('/cpu:0'):\n",
    "        with tf.name_scope('embeddings'):\n",
    "            # initialize the the biased embedding weights\n",
    "            embeddings_wv = tf.Variable(trained_wv_weights)\n",
    "            embeddings_cv = tf.Variable(trained_cv_weights)\n",
    "        embed_wv = tf.nn.embedding_lookup(embeddings_wv,indices_wv_inputs)\n",
    "        embed_cv = tf.nn.embedding_lookup(embeddings_cv,indices_cv_inputs)\n",
    "        embed_sv = tf.nn.embedding_lookup(embeddings_wv,samples_inputs)\n",
    "        prod = tf.matmul(embed_wv, tf.transpose(embed_cv))\n",
    "        first_term = tf.math.log_sigmoid(prod)\n",
    "        embed_sv = tf.reshape(embed_sv,[vector_dim,batch_size,k])\n",
    "        prod_2 = tf.tensordot(embed_cv, -1*embed_sv, axes=[[1],[0]])\n",
    "        prod_2_log_sig = tf.math.log_sigmoid(prod_2)\n",
    "        second_term = tf.reduce_sum(prod_2_log_sig,2)\n",
    "        pred_vals = first_term + second_term\n",
    "    with tf.name_scope('loss'):\n",
    "        log_val = tf.log(y_train_inputs + eps)\n",
    "        loss = tf.reduce_sum(tf.abs(log_val - pred_vals))\n",
    "    \n",
    "    tf.summary.scalar('loss', loss)\n",
    "    \n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(0.0003).minimize(loss)\n",
    "    \n",
    "#     norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "#     normalized_embeddings = embeddings / norm\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_wv, indices_cv, samples, yt = generate_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167.31906\n"
     ]
    }
   ],
   "source": [
    "feed_dict = {indices_wv_inputs: indices_wv, indices_cv_inputs: indices_cv, samples_inputs: samples, y_train_inputs: yt}\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    res = sess.run(loss, feed_dict)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "('Average loss at step ', 0, ': ', 118.79475402832031)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 2000, ': ', 166.43611096572877)\n",
      "New min loss, saving embedding\n",
      "('Average loss at step ', 4000, ': ', 159.29424289131165)\n",
      "New min loss, saving embedding\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-431-2494dd791c31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         _, summary, loss_val = session.run([optimizer, merged, loss],\n\u001b[1;32m     21\u001b[0m                                          \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                                          run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0maverage_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haileyjames/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haileyjames/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haileyjames/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haileyjames/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haileyjames/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haileyjames/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 1000001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "# Open a writer to write summaries.\n",
    "\n",
    "# We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "    min_loss = sys.maxsize\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        indices_wv, indices_cv, samples, yt = generate_batch(batch_size)\n",
    "        feed_dict = {indices_wv_inputs: indices_wv, indices_cv_inputs: indices_cv, samples_inputs: samples, y_train_inputs: yt}\n",
    "\n",
    "\n",
    "\n",
    "      # Define metadata variable.\n",
    "        run_metadata = tf.RunMetadata()\n",
    "\n",
    "        _, summary, loss_val = session.run([optimizer, merged, loss],\n",
    "                                         feed_dict=feed_dict,\n",
    "                                         run_metadata=run_metadata)\n",
    "        average_loss += loss_val\n",
    "\n",
    "      # Add returned summaries to writer in each step.\n",
    "      # Add metadata to visualize the graph for the last run.\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "        # The average loss is an estimate of the loss over the last 2000\n",
    "        # batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            if average_loss < min_loss:\n",
    "                print('New min loss, saving embedding')\n",
    "                wvs_embed_trained = embeddings_wv.eval()\n",
    "\n",
    "                cvs_embed_trained = embeddings_cv.eval()\n",
    "                \n",
    "            average_loss = 0\n",
    "\n",
    "    wvs_embed_trained = embeddings_wv.eval()\n",
    "\n",
    "    cvs_embed_trained = embeddings_cv.eval()\n",
    "\n",
    "#     final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "    # Save the model for checkpoints.\n",
    "    saver.save(session, './model_0')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('wvs_embed_trained_3', wvs_embed_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('cvs_embed_trained_3', cvs_embed_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvs_embed_trained_norm_3 = np.sqrt(np.sum(np.square(wvs_embed_trained_3), 1, keepdims=True))\n",
    "wvs_embed_trained_normalized_embeddings_3 = wvs_embed_trained_2 / wvs_embed_trained_norm_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('wvs_embed_trained_normalized_embeddings_3', wvs_embed_trained_normalized_embeddings_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
