{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Must be run in Python 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Dense, Input, Reshape, Flatten, dot, Add\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_model = Word2Vec.load(\"english-wikipedia-articles-20170820-models/enwiki_2017_08_20_fasttext.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 22000\n",
    "vector_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_np_to_bin_model(np_vectors,model_name):\n",
    "    with open(model_name+'.txt', 'w') as we:\n",
    "        we.write('{} {}\\n'.format(vocab_size,vector_dim))\n",
    "        for i in range(vocab_size):\n",
    "            w = wiki_model.wv.index2word[i]\n",
    "            vec = np_vectors[i]\n",
    "            we.write('{} '.format(w))\n",
    "            for v in vec:\n",
    "                we.write(str(v) + ' ')\n",
    "            we.write('\\n')\n",
    "    model = KeyedVectors.load_word2vec_format(model_name+'.txt', binary=False)\n",
    "    model.save_word2vec_format(model_name+'.bin', binary=True)\n",
    "    print('created model '+model_name+'.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvs_embed_trained_normalized_embeddings = np.load('wvs_embed_trained_normalized_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created model wvs_embed_trained_normalized_embeddings_2.bin\n"
     ]
    }
   ],
   "source": [
    "convert_np_to_bin_model(wvs_embed_trained_normalized_embeddings,'wvs_embed_trained_normalized_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_embed_small.bin\n",
      "12:51:34 INFO:loading projection weights from /Users/haileyjames/Documents/wasserstein-debiasing/original_embed_small.bin\n",
      "12:51:34 INFO:Loading #22000 words with 100 dim\n",
      "12:51:34 INFO:Transformed 22000 into 22000 words\n",
      "12:51:34 INFO:Calculating similarity benchmarks\n",
      "12:51:34 WARNING:Missing 531 words. Will replace them with mean vector\n",
      "12:51:34 INFO:Spearman correlation of scores on MEN 0.5983153481584027\n",
      "12:51:34 WARNING:Missing 48 words. Will replace them with mean vector\n",
      "12:51:34 INFO:Spearman correlation of scores on WS353 0.46266864889490805\n",
      "12:51:34 WARNING:Missing 27 words. Will replace them with mean vector\n",
      "12:51:34 INFO:Spearman correlation of scores on WS353R 0.4104594294511796\n",
      "12:51:34 WARNING:Missing 27 words. Will replace them with mean vector\n",
      "12:51:34 INFO:Spearman correlation of scores on WS353S 0.5393883796332126\n",
      "12:51:34 WARNING:Missing 93 words. Will replace them with mean vector\n",
      "12:51:34 INFO:Spearman correlation of scores on SimLex999 0.2548159994096536\n",
      "12:51:34 WARNING:Missing 2204 words. Will replace them with mean vector\n",
      "12:51:34 INFO:Spearman correlation of scores on RW 0.1839123605991901\n",
      "12:51:34 WARNING:Missing 15 words. Will replace them with mean vector\n",
      "12:51:34 INFO:Spearman correlation of scores on RG65 0.5179049538220439\n",
      "12:51:34 WARNING:Missing 19 words. Will replace them with mean vector\n",
      "12:51:34 INFO:Spearman correlation of scores on MTurk 0.6029118494702651\n",
      "12:51:34 WARNING:Missing 13500 words. Will replace them with mean vector\n",
      "12:51:35 INFO:Spearman correlation of scores on TR9856 0.13024238042173106\n",
      "12:51:35 INFO:Calculating analogy benchmarks\n",
      "12:51:35 WARNING:Missing 7702 words. Will replace them with mean vector\n",
      "12:51:35 INFO:Processing 1/196 batch\n",
      "12:51:36 INFO:Processing 20/196 batch\n",
      "12:51:37 INFO:Processing 39/196 batch\n",
      "12:51:37 INFO:Processing 58/196 batch\n",
      "12:51:37 INFO:Processing 77/196 batch\n",
      "12:51:38 INFO:Processing 96/196 batch\n",
      "12:51:38 INFO:Processing 115/196 batch\n",
      "12:51:39 INFO:Processing 134/196 batch\n",
      "12:51:40 INFO:Processing 153/196 batch\n",
      "12:51:40 INFO:Processing 172/196 batch\n",
      "12:51:41 INFO:Processing 191/196 batch\n",
      "12:51:41 INFO:Analogy prediction accuracy on Google 0.44683790421612773\n",
      "12:51:41 WARNING:Missing 4430 words. Will replace them with mean vector\n",
      "12:51:41 INFO:Processing 1/80 batch\n",
      "12:51:42 INFO:Processing 9/80 batch\n",
      "12:51:42 INFO:Processing 17/80 batch\n",
      "12:51:42 INFO:Processing 25/80 batch\n",
      "12:51:42 INFO:Processing 33/80 batch\n",
      "12:51:43 INFO:Processing 41/80 batch\n",
      "12:51:43 INFO:Processing 49/80 batch\n",
      "12:51:43 INFO:Processing 57/80 batch\n",
      "12:51:43 INFO:Processing 65/80 batch\n",
      "12:51:44 INFO:Processing 73/80 batch\n",
      "12:51:44 INFO:Analogy prediction accuracy on MSR 0.4745\n",
      "/Users/haileyjames/anaconda2/envs/p3/lib/python3.6/site-packages/scipy/stats/stats.py:245: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  \"values. nan values will be ignored.\", RuntimeWarning)\n",
      "12:51:44 INFO:Analogy prediction accuracy on SemEval2012 0.17922905389850294\n",
      "12:51:44 INFO:Calculating categorization benchmarks\n",
      "12:51:45 DEBUG:Purity=0.448 using affinity=euclidean linkage=ward\n",
      "12:51:45 DEBUG:Purity=0.316 using affinity=cosine linkage=average\n",
      "12:51:45 DEBUG:Purity=0.423 using affinity=cosine linkage=complete\n",
      "12:51:45 DEBUG:Purity=0.259 using affinity=euclidean linkage=average\n",
      "12:51:45 DEBUG:Purity=0.396 using affinity=euclidean linkage=complete\n",
      "12:51:45 DEBUG:Purity=0.458 using KMeans\n",
      "12:51:45 INFO:Cluster purity on AP 0.4577114427860696\n",
      "12:51:45 DEBUG:Purity=0.575 using affinity=euclidean linkage=ward\n",
      "12:51:45 DEBUG:Purity=0.450 using affinity=cosine linkage=average\n",
      "12:51:45 DEBUG:Purity=0.520 using affinity=cosine linkage=complete\n",
      "12:51:45 DEBUG:Purity=0.330 using affinity=euclidean linkage=average\n",
      "12:51:45 DEBUG:Purity=0.450 using affinity=euclidean linkage=complete\n",
      "12:51:45 DEBUG:Purity=0.580 using KMeans\n",
      "12:51:45 INFO:Cluster purity on BLESS 0.58\n",
      "12:51:47 DEBUG:Purity=0.292 using affinity=euclidean linkage=ward\n",
      "12:51:49 DEBUG:Purity=0.169 using affinity=cosine linkage=average\n",
      "12:51:51 DEBUG:Purity=0.242 using affinity=cosine linkage=complete\n",
      "12:51:53 DEBUG:Purity=0.097 using affinity=euclidean linkage=average\n",
      "12:51:55 DEBUG:Purity=0.242 using affinity=euclidean linkage=complete\n",
      "12:51:56 DEBUG:Purity=0.297 using KMeans\n",
      "12:51:56 INFO:Cluster purity on Battig 0.29707512903842476\n",
      "12:51:56 DEBUG:Purity=0.644 using affinity=euclidean linkage=ward\n",
      "12:51:56 DEBUG:Purity=0.533 using affinity=cosine linkage=average\n",
      "12:51:56 DEBUG:Purity=0.511 using affinity=cosine linkage=complete\n",
      "12:51:56 DEBUG:Purity=0.533 using affinity=euclidean linkage=average\n",
      "12:51:56 DEBUG:Purity=0.511 using affinity=euclidean linkage=complete\n",
      "12:51:56 DEBUG:Purity=0.600 using KMeans\n",
      "12:51:56 INFO:Cluster purity on ESSLI_2c 0.6444444444444445\n",
      "12:51:56 DEBUG:Purity=0.725 using affinity=euclidean linkage=ward\n",
      "12:51:56 DEBUG:Purity=0.500 using affinity=cosine linkage=average\n",
      "12:51:56 DEBUG:Purity=0.725 using affinity=cosine linkage=complete\n",
      "12:51:56 DEBUG:Purity=0.525 using affinity=euclidean linkage=average\n",
      "12:51:56 DEBUG:Purity=0.725 using affinity=euclidean linkage=complete\n",
      "12:51:56 DEBUG:Purity=0.750 using KMeans\n",
      "12:51:56 INFO:Cluster purity on ESSLI_2b 0.75\n",
      "12:51:56 DEBUG:Purity=0.682 using affinity=euclidean linkage=ward\n",
      "12:51:56 DEBUG:Purity=0.523 using affinity=cosine linkage=average\n",
      "12:51:56 DEBUG:Purity=0.636 using affinity=cosine linkage=complete\n",
      "12:51:56 DEBUG:Purity=0.477 using affinity=euclidean linkage=average\n",
      "12:51:56 DEBUG:Purity=0.636 using affinity=euclidean linkage=complete\n",
      "12:51:56 DEBUG:Purity=0.659 using KMeans\n",
      "12:51:56 INFO:Cluster purity on ESSLI_1a 0.6818181818181819\n",
      "12:51:56 INFO:Saving results...\n",
      "         AP  BLESS    Battig  ...    Google     MSR  SemEval2012_2\n",
      "0  0.457711   0.58  0.297075  ...  0.446838  0.4745       0.179229\n",
      "\n",
      "[1 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "! ../word-embeddings-benchmarks/scripts/evaluate_on_all_orig.py -f original_embed_small.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wvs_embed_trained_normalized_embeddings.bin\n",
      "09:07:23 INFO:loading projection weights from /Users/haileyjames/Documents/wasserstein-debiasing/wvs_embed_trained_normalized_embeddings.bin\n",
      "09:07:23 INFO:Loading #22000 words with 100 dim\n",
      "09:07:23 INFO:Transformed 22000 into 22000 words\n",
      "09:07:23 INFO:Calculating similarity benchmarks\n",
      "09:07:23 WARNING:Missing 531 words. Will replace them with mean vector\n",
      "09:07:23 INFO:Spearman correlation of scores on MEN 0.5775930360006853\n",
      "09:07:23 WARNING:Missing 48 words. Will replace them with mean vector\n",
      "09:07:23 INFO:Spearman correlation of scores on WS353 0.3902896158576897\n",
      "09:07:23 WARNING:Missing 27 words. Will replace them with mean vector\n",
      "09:07:23 INFO:Spearman correlation of scores on WS353R 0.3472629665223466\n",
      "09:07:23 WARNING:Missing 27 words. Will replace them with mean vector\n",
      "09:07:23 INFO:Spearman correlation of scores on WS353S 0.4755790923001263\n",
      "09:07:23 WARNING:Missing 93 words. Will replace them with mean vector\n",
      "09:07:23 INFO:Spearman correlation of scores on SimLex999 0.24014198684880345\n",
      "09:07:23 WARNING:Missing 2204 words. Will replace them with mean vector\n",
      "09:07:23 INFO:Spearman correlation of scores on RW 0.13160046202193387\n",
      "09:07:23 WARNING:Missing 15 words. Will replace them with mean vector\n",
      "09:07:23 INFO:Spearman correlation of scores on RG65 0.5217296306553502\n",
      "09:07:23 WARNING:Missing 19 words. Will replace them with mean vector\n",
      "09:07:23 INFO:Spearman correlation of scores on MTurk 0.5514993300544669\n",
      "09:07:23 WARNING:Missing 13500 words. Will replace them with mean vector\n",
      "09:07:24 INFO:Spearman correlation of scores on TR9856 0.11225298081148254\n",
      "09:07:24 INFO:Calculating analogy benchmarks\n",
      "09:07:24 WARNING:Missing 7702 words. Will replace them with mean vector\n",
      "09:07:24 INFO:Processing 1/196 batch\n",
      "09:07:25 INFO:Processing 20/196 batch\n",
      "09:07:25 INFO:Processing 39/196 batch\n",
      "09:07:26 INFO:Processing 58/196 batch\n",
      "09:07:26 INFO:Processing 77/196 batch\n",
      "09:07:27 INFO:Processing 96/196 batch\n",
      "09:07:27 INFO:Processing 115/196 batch\n",
      "09:07:27 INFO:Processing 134/196 batch\n",
      "09:07:28 INFO:Processing 153/196 batch\n",
      "09:07:28 INFO:Processing 172/196 batch\n",
      "09:07:28 INFO:Processing 191/196 batch\n",
      "09:07:28 INFO:Analogy prediction accuracy on Google 0.42432460090053215\n",
      "09:07:28 WARNING:Missing 4430 words. Will replace them with mean vector\n",
      "09:07:28 INFO:Processing 1/80 batch\n",
      "09:07:29 INFO:Processing 9/80 batch\n",
      "09:07:29 INFO:Processing 17/80 batch\n",
      "09:07:29 INFO:Processing 25/80 batch\n",
      "09:07:29 INFO:Processing 33/80 batch\n",
      "09:07:29 INFO:Processing 41/80 batch\n",
      "09:07:29 INFO:Processing 49/80 batch\n",
      "09:07:30 INFO:Processing 57/80 batch\n",
      "09:07:30 INFO:Processing 65/80 batch\n",
      "09:07:30 INFO:Processing 73/80 batch\n",
      "09:07:30 INFO:Analogy prediction accuracy on MSR 0.443125\n",
      "/Users/haileyjames/anaconda2/envs/p3/lib/python3.6/site-packages/scipy/stats/stats.py:245: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  \"values. nan values will be ignored.\", RuntimeWarning)\n",
      "09:07:31 INFO:Analogy prediction accuracy on SemEval2012 0.18342607769828387\n",
      "09:07:31 INFO:Calculating categorization benchmarks\n",
      "09:07:31 DEBUG:Purity=0.450 using affinity=euclidean linkage=ward\n",
      "09:07:31 DEBUG:Purity=0.286 using affinity=cosine linkage=average\n",
      "09:07:31 DEBUG:Purity=0.376 using affinity=cosine linkage=complete\n",
      "09:07:31 DEBUG:Purity=0.159 using affinity=euclidean linkage=average\n",
      "09:07:31 DEBUG:Purity=0.378 using affinity=euclidean linkage=complete\n",
      "09:07:31 DEBUG:Purity=0.443 using KMeans\n",
      "09:07:31 INFO:Cluster purity on AP 0.4502487562189055\n",
      "09:07:32 DEBUG:Purity=0.545 using affinity=euclidean linkage=ward\n",
      "09:07:32 DEBUG:Purity=0.440 using affinity=cosine linkage=average\n",
      "09:07:32 DEBUG:Purity=0.480 using affinity=cosine linkage=complete\n",
      "09:07:32 DEBUG:Purity=0.350 using affinity=euclidean linkage=average\n",
      "09:07:32 DEBUG:Purity=0.505 using affinity=euclidean linkage=complete\n",
      "09:07:32 DEBUG:Purity=0.560 using KMeans\n",
      "09:07:32 INFO:Cluster purity on BLESS 0.56\n",
      "09:07:34 DEBUG:Purity=0.291 using affinity=euclidean linkage=ward\n",
      "09:07:36 DEBUG:Purity=0.155 using affinity=cosine linkage=average\n",
      "09:07:37 DEBUG:Purity=0.250 using affinity=cosine linkage=complete\n",
      "09:07:39 DEBUG:Purity=0.085 using affinity=euclidean linkage=average\n",
      "09:07:41 DEBUG:Purity=0.233 using affinity=euclidean linkage=complete\n",
      "09:07:42 DEBUG:Purity=0.295 using KMeans\n",
      "09:07:42 INFO:Cluster purity on Battig 0.2949722806346779\n",
      "09:07:42 DEBUG:Purity=0.622 using affinity=euclidean linkage=ward\n",
      "09:07:42 DEBUG:Purity=0.489 using affinity=cosine linkage=average\n",
      "09:07:42 DEBUG:Purity=0.600 using affinity=cosine linkage=complete\n",
      "09:07:42 DEBUG:Purity=0.533 using affinity=euclidean linkage=average\n",
      "09:07:42 DEBUG:Purity=0.600 using affinity=euclidean linkage=complete\n",
      "09:07:42 DEBUG:Purity=0.533 using KMeans\n",
      "09:07:42 INFO:Cluster purity on ESSLI_2c 0.6222222222222222\n",
      "09:07:42 DEBUG:Purity=0.700 using affinity=euclidean linkage=ward\n",
      "09:07:42 DEBUG:Purity=0.775 using affinity=cosine linkage=average\n",
      "09:07:42 DEBUG:Purity=0.575 using affinity=cosine linkage=complete\n",
      "09:07:42 DEBUG:Purity=0.575 using affinity=euclidean linkage=average\n",
      "09:07:42 DEBUG:Purity=0.575 using affinity=euclidean linkage=complete\n",
      "09:07:43 DEBUG:Purity=0.800 using KMeans\n",
      "09:07:43 INFO:Cluster purity on ESSLI_2b 0.8\n",
      "09:07:43 DEBUG:Purity=0.682 using affinity=euclidean linkage=ward\n",
      "09:07:43 DEBUG:Purity=0.477 using affinity=cosine linkage=average\n",
      "09:07:43 DEBUG:Purity=0.591 using affinity=cosine linkage=complete\n",
      "09:07:43 DEBUG:Purity=0.477 using affinity=euclidean linkage=average\n",
      "09:07:43 DEBUG:Purity=0.523 using affinity=euclidean linkage=complete\n",
      "09:07:43 DEBUG:Purity=0.659 using KMeans\n",
      "09:07:43 INFO:Cluster purity on ESSLI_1a 0.6818181818181819\n",
      "09:07:43 INFO:Saving results...\n",
      "         AP  BLESS    Battig  ...    Google       MSR  SemEval2012_2\n",
      "0  0.450249   0.56  0.294972  ...  0.424325  0.443125       0.183426\n",
      "\n",
      "[1 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "! ../word-embeddings-benchmarks/scripts/evaluate_on_all_orig.py -f wvs_embed_trained_normalized_embeddings.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change `compare_embeddings_gender.config` to have the name of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Embedding\n",
      "+-------------------------------------------------------+-------+-------+\n",
      "|                         Test                          | Mean  | Error |\n",
      "+-------------------------------------------------------+-------+-------+\n",
      "| Flowers vs Insects (25) / Pleasant vs Unpleasant (25) | 1.395 | 0.036 |\n",
      "+-------------------------------------------------------+-------+-------+\n",
      "|   Instruments vs Weapons /  Pleasant vs Unpleasant    | 1.505 | 0.015 |\n",
      "+-------------------------------------------------------+-------+-------+\n",
      "|         Male8 vs Female8 /  Career vs Family          | 1.052 | 0.042 |\n",
      "+-------------------------------------------------------+-------+-------+\n",
      "|            Math vs Art / Male8 vs Female8             | 1.061 | 0.061 |\n",
      "+-------------------------------------------------------+-------+-------+\n",
      "|           Science vs Art / Male8 vs Female8           | 1.204 | 0.064 |\n",
      "+-------------------------------------------------------+-------+-------+\n",
      "Debiased Embedding\n",
      "+-------------------------------------------------------+--------+-------+\n",
      "|                         Test                          |  Mean  | Error |\n",
      "+-------------------------------------------------------+--------+-------+\n",
      "| Flowers vs Insects (25) / Pleasant vs Unpleasant (25) | 1.157  | 0.05  |\n",
      "+-------------------------------------------------------+--------+-------+\n",
      "|   Instruments vs Weapons /  Pleasant vs Unpleasant    | 1.567  | 0.016 |\n",
      "+-------------------------------------------------------+--------+-------+\n",
      "|         Male8 vs Female8 /  Career vs Family          | 0.984  | 0.044 |\n",
      "+-------------------------------------------------------+--------+-------+\n",
      "|            Math vs Art / Male8 vs Female8             | -0.048 | 0.105 |\n",
      "+-------------------------------------------------------+--------+-------+\n",
      "|           Science vs Art / Male8 vs Female8           | 0.802  | 0.071 |\n",
      "+-------------------------------------------------------+--------+-------+\n"
     ]
    }
   ],
   "source": [
    "!python ../compare-embedding-bias/weat.py ../compare-embedding-bias/configs/compare_embeddings_gender.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
